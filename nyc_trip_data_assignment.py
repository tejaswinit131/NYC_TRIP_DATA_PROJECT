# -*- coding: utf-8 -*-
"""nyc_trip_data_assignment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zw_-ovjCjxuaOrTr8wINcYxKTRBaF_CK

**NYC Trip Data**

Project Overview

This project processes New York Taxi Trip data for the year 2019 to derive analytical insights and load the processed data into a SQLite database for further analysis.

Environment Setup

Ensure the following Python packages are installed:

Python 3.7+,

Pandas,

Requests,

TQDM,

SQLite3,

Matplotlib,

Seaborn

---
"""

!pip install requests
!pip install bs4
!pip install fastparquet
!pip install pandas

"""Project Structure

The project contains the following scripts:

Extraction Part

NYC_TRIP_DATA_EXTRACTION

I extracted taxi trip data for three months in 2019. Attempting to process data for the entire year or six months resulted in a fatal error: "The Python kernel is unresponsive."
Due to system limitations, I couldn't execute the extraction for a year and for six months. So , i took one month of trip_data which is easily processed in my system.

Finally, I extracted data for one month and mount to google drive.

ERROR_HANDLING:

I handle network errors and retry error also .
The combination of requests.Session() and Retry from urllib3 provides a robust way to handle network errors and automatically retry requests. This setup helps ensure that transient errors do not cause our script to fail, making the download process more reliable.
"""

import os
import requests
from bs4 import BeautifulSoup
from time import sleep
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import pandas as pd
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Create a session to handle retries
session = requests.Session()
retries = Retry(total=5, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
session.mount('http://', HTTPAdapter(max_retries=retries))
session.mount('https://', HTTPAdapter(max_retries=retries))

# Define the base URL for scraping and the directory in Google Drive to save the files
scrape_url = "https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
drive_dir = "/content/drive/My Drive/nyc_taxi_data_2019"

# Create the directory in Google Drive if it doesn't exist
os.makedirs(drive_dir, exist_ok=True)

# Function to download a file with retries
def download_file(url, output_path):
    try:
        response = session.get(url, stream=True)
        response.raise_for_status()  # Raise an error for bad status codes
        with open(output_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
        print(f"Successfully downloaded {output_path}")
    except requests.exceptions.RequestException as e:
        print(f"Failed to download {url}: {e}")

# Scrape the website to find the correct URLs
response = session.get(scrape_url)
soup = BeautifulSoup(response.text, 'html.parser')
links = soup.find_all('a', href=True)

# Filter and download parquet files for the first six months of 2019
parquet_files = []
months = ['01']
for link in links:
    href = link['href']
    if any(f'2019-{month}' in href for month in months) and href.endswith('.parquet'):
        filename = href.split('/')[-1]
        url = href
        output_path = os.path.join(drive_dir, filename)
        download_file(url, output_path)
        parquet_files.append(output_path)
        sleep(1)  # Sleep for a short time between requests to avoid overloading the server

print("Download completed.")

"""**Processing Part**

NYC_TRIP_DATA_PROCESSING

After extracting the parquet files, I checked their schemas using df.printSchema(). The "For-Hire Vehicle Trip Records" files lacked the required columns, so I skipped these files and applied transformations to the remaining ones.

Transformations included:

Cleaning and transforming the data for analysis. Removing trips with missing or corrupt data. Deriving new columns such as trip duration and average speed. Aggregating data to calculate total trips and average fare per day. I displayed five records of each transformed DataFrame.

Loading Part

I loaded processed data into a SQLite database, creating necessary tables and indexes.
And I fillfulled the following requirments ,
Design and implement a schema suitable for querying trip metrics.
Use SQL to load data into the database efficiently.
And the result is shown below.


"""

!pip install pyspark

import os
import pandas as pd
from pyspark.sql import SparkSession, functions as F

# Initialize Spark session
spark = SparkSession.builder \
    .appName("TaxiDataProcessing") \
    .getOrCreate()

def clean_and_transform_spark(df, taxi_type):
    if taxi_type == "yellow":
        required_columns = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance', 'fare_amount']
        df = df.dropna(subset=required_columns)
        df = df.withColumn('pickup_datetime', F.to_timestamp('tpep_pickup_datetime')) \
               .withColumn('dropoff_datetime', F.to_timestamp('tpep_dropoff_datetime'))
    elif taxi_type == "green":
        required_columns = ['lpep_pickup_datetime', 'lpep_dropoff_datetime', 'trip_distance', 'fare_amount']
        df = df.dropna(subset=required_columns)
        df = df.withColumn('pickup_datetime', F.to_timestamp('lpep_pickup_datetime')) \
               .withColumn('dropoff_datetime', F.to_timestamp('lpep_dropoff_datetime'))
    '''elif taxi_type == "hvfht":
        required_columns = ['pickup_datetime', 'dropoff_datetime', 'trip_miles', 'base_passenger_fare']
        df = df.dropna(subset=required_columns)
        df = df.withColumn('pickup_datetime', F.to_timestamp('pickup_datetime')) \
               .withColumn('dropoff_datetime', F.to_timestamp('dropoff_datetime'))'''

    df = df.withColumn('trip_duration',
                       (F.col('dropoff_datetime').cast('long') - F.col('pickup_datetime').cast('long')) / 60) \
           .withColumn('average_speed', F.col('trip_distance') / (F.col('trip_duration') / 60))

    return df

# Define paths
use_google_drive = True  # Set to False if using local upload
if use_google_drive:
    base_path = '/content/drive/My Drive/nyc_taxi_data_2019'# Update with your Google Drive path
else:
    base_path = '/content/'  # Default path for local uploads

parquet_files = [
    'file1.parquet',
    'file2.parquet'
]  # Update with your actual parquet file names

# Read, clean, and display data from each parquet file
for local_file in parquet_files:
    file_path = os.path.join(base_path, local_file)
    taxi_type = ""
    if "yellow" in file_path:
        taxi_type = "yellow"
    elif "green" in file_path:
        taxi_type = "green"
    '''elif "hvfht" in file_path:
        taxi_type = "hvfht"'''

    if taxi_type:
        try:
            df = spark.read.parquet(file_path)
            df = clean_and_transform_spark(df, taxi_type)
            df.show(5)
        except Exception as e:
            print(f"Error processing file {file_path}: {e}")

print("Data loaded, cleaned, and displayed.")

import os
import pandas as pd
from datetime import datetime

# Define local directory
local_dir = "/content/drive/My Drive/nyc_taxi_data_2019"

# Function to process data
def process_data(file_path, pickup_col, dropoff_col, distance_col, fare_col):
    # Read parquet file into DataFrame
    df = pd.read_parquet(file_path)

    # Select necessary columns and drop missing values
    df = df[[pickup_col, dropoff_col, distance_col, fare_col]].dropna()

    # Rename columns
    df.columns = ['pickup_datetime', 'dropoff_datetime', 'trip_distance', 'fare_amount']

    # Convert datetime columns
    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])
    df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])

    # Derive new columns: trip duration (minutes) and average speed (mph)
    df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60
    df['average_speed'] = df['trip_distance'] / (df['trip_duration'] / 60)

    # Remove invalid data
    df = df[(df['trip_duration'] > 0) & (df['average_speed'].notnull())]

    # Aggregate data: total trips and average fare per day
    df['pickup_date'] = df['pickup_datetime'].dt.date
    agg_df = df.groupby('pickup_date').agg(
        total_trips=('trip_distance', 'count'),
        average_fare=('fare_amount', 'mean')
    ).reset_index()

    return df, agg_df

# Process each file and display results
for month in ['01']:
    for taxi_type, pickup_col, dropoff_col, distance_col, fare_col in [
        ('yellow', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance', 'fare_amount'),
        ('green', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'trip_distance', 'fare_amount'),
        # ('fhvhv', 'pickup_datetime', 'dropoff_datetime', 'trip_miles', 'base_passenger_fare')  # Removed extra values if any
    ]:
        # Get file path
        file_path = os.path.join(local_dir, f'{taxi_type}_tripdata_2019-{month}.parquet')

        # Process data
        try:
            df, agg_df = process_data(file_path, pickup_col, dropoff_col, distance_col, fare_col)

            # Show DataFrame description
            print(f"Description of {taxi_type} data for 2019-{month}:")
            print(df.describe())

            # Show aggregated results
            print(agg_df.head())
        except Exception as e:
            print(f"Error processing {file_path}: {e}")

print("Data processing completed.")

import os
import pandas as pd
import sqlite3
from datetime import datetime

# Define local directory and SQLite database file
local_dir = "/content/drive/My Drive/nyc_taxi_data_2019"
db_file = "nyc_taxi_data_2019.db"

# Function to process data
def process_data(file_path, pickup_col, dropoff_col, distance_col, fare_col, passenger_col, taxi_type):
    # Read parquet file into DataFrame
    df = pd.read_parquet(file_path)

    # Select necessary columns and drop missing values
    df = df[[pickup_col, dropoff_col, distance_col, fare_col, passenger_col]].dropna()

    # Rename columns
    df.columns = ['pickup_datetime', 'dropoff_datetime', 'trip_distance', 'fare_amount', 'passenger_count']

    # Convert datetime columns
    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])
    df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])

    # Derive new columns: trip duration (minutes) and average speed (mph)
    df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60
    df['average_speed'] = df['trip_distance'] / (df['trip_duration'] / 60)

    # Remove invalid data
    df = df[(df['trip_duration'] > 0) & (df['average_speed'].notnull())]

    # Add taxi type column
    df['taxi_type'] = taxi_type

    # Aggregate data: total trips, average fare, and total passengers per day
    df['pickup_date'] = df['pickup_datetime'].dt.date
    agg_df = df.groupby('pickup_date').agg(
        total_trips=('trip_distance', 'count'),
        average_fare=('fare_amount', 'mean'),
        total_passengers=('passenger_count', 'sum')
    ).reset_index()

    agg_df['taxi_type'] = taxi_type

    # Drop the pickup_date column from the main dataframe to avoid insertion errors
    df = df.drop(columns=['pickup_date'])

    return df, agg_df

# Function to load data into SQLite
def load_to_sqlite(df, table_name, conn):
    df.to_sql(table_name, conn, if_exists='append', index=False)

# Create a connection to the SQLite database
conn = sqlite3.connect(db_file)
cursor = conn.cursor()

# Drop tables if they exist (optional)
cursor.executescript('''
DROP TABLE IF EXISTS Trips;
DROP TABLE IF EXISTS DailyMetrics;
''')

# Create tables
cursor.executescript('''
-- Create Trips Table
CREATE TABLE IF NOT EXISTS Trips (
    trip_id INTEGER PRIMARY KEY AUTOINCREMENT,
    taxi_type TEXT,
    pickup_datetime TIMESTAMP,
    dropoff_datetime TIMESTAMP,
    trip_distance REAL,
    fare_amount REAL,
    passenger_count INTEGER,
    trip_duration REAL,
    average_speed REAL
);

-- Create DailyMetrics Table
CREATE TABLE IF NOT EXISTS DailyMetrics (
    metric_id INTEGER PRIMARY KEY AUTOINCREMENT,
    taxi_type TEXT,
    pickup_date DATE,
    total_trips INTEGER,
    average_fare REAL,
    total_passengers INTEGER
);
''')

# Process each file and load data into SQLite
for month in ['01']:
    for taxi_type, pickup_col, dropoff_col, distance_col, fare_col, passenger_col in [
        ('yellow', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance', 'fare_amount', 'passenger_count'),
        ('green', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'trip_distance', 'fare_amount', 'passenger_count'),
        # ('fhvhv', 'pickup_datetime', 'dropoff_datetime', 'trip_miles', 'base_passenger_fare', 'passenger_count')
    ]:
        # Get file path
        file_path = os.path.join(local_dir, f'{taxi_type}_tripdata_2019-{month}.parquet')

        # Process data
        if os.path.exists(file_path):
            try:
                df, agg_df = process_data(file_path, pickup_col, dropoff_col, distance_col, fare_col, passenger_col, taxi_type)

                # Load processed data into SQLite
                load_to_sqlite(df, 'Trips', conn)

                # Load aggregated data into SQLite
                load_to_sqlite(agg_df, 'DailyMetrics', conn)

                print(f"Data for {taxi_type} 2019-{month} loaded successfully.")
            except Exception as e:
                print(f"Error processing {file_path}: {e}")
        else:
            print(f"File {file_path} does not exist.")

# Commit and close the SQLite connection
conn.commit()
conn.close()

print("Data processing and loading to SQLite completed.")

"""**Analysis Part**

NYC_TRIP_DATA_ANALYSIS

Data Analysis and Reporting

Task: Generate insights and reports from the database.

Requirements:

Develop SQL queries to answer the following questions:
    What are the peak hours for taxi usage?
    How does passenger count affect the trip fare?
    What are the trends in usage over the year?
Create visualizations to represent the findings.
"""

import sqlite3
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Connect to the SQLite database
db_file = "nyc_taxi_data_2019.db"
conn = sqlite3.connect(db_file)

# SQL Query 1: Peak hours for taxi usage
peak_hours_query = """
SELECT strftime('%H', pickup_datetime) AS hour, COUNT(*) AS total_trips
FROM Trips
GROUP BY hour
ORDER BY total_trips DESC
"""
peak_hours_df = pd.read_sql_query(peak_hours_query, conn)

# SQL Query 2: Passenger count effect on trip fare (assuming passenger_count column exists)
passenger_fare_query = """
SELECT passenger_count, AVG(fare_amount) AS average_fare
FROM Trips
GROUP BY passenger_count
ORDER BY passenger_count
"""
passenger_fare_df = pd.read_sql_query(passenger_fare_query, conn) # Uncomment this line to execute the query

# SQL Query 3: Trends in usage over the year
usage_trends_query = """
SELECT strftime('%Y-%m', pickup_datetime) AS month, COUNT(*) AS total_trips
FROM Trips
GROUP BY month
ORDER BY month
"""
usage_trends_df = pd.read_sql_query(usage_trends_query, conn)

# Close the connection
conn.close()

# Plot 1: Peak hours for taxi usage
plt.figure(figsize=(12, 6))
sns.barplot(x='hour', y='total_trips', data=peak_hours_df, palette='viridis')
plt.title('Peak Hours for Taxi Usage')
plt.xlabel('Hour of the Day')
plt.ylabel('Total Trips')
plt.xticks(rotation=45)
plt.show()

# Plot 2: Passenger count effect on trip fare
plt.figure(figsize=(12, 6))
sns.barplot(x='passenger_count', y='average_fare', data=passenger_fare_df, palette='coolwarm')
plt.title('Effect of Passenger Count on Trip Fare')
plt.xlabel('Passenger Count')
plt.ylabel('Average Fare ($)')
plt.show()

# Plot 3: Trends in usage over the year
plt.figure(figsize=(12, 6))
sns.lineplot(x='month', y='total_trips', data=usage_trends_df, marker='o', color='b')
plt.title('Trends in Taxi Usage Over the Year')
plt.xlabel('Month')
plt.ylabel('Total Trips')
plt.xticks(rotation=45)
plt.show()

print(df.describe())

print(df.info())
print(df.describe())