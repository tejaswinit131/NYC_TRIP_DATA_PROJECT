{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "from datetime import datetime\n",
        "\n",
        "# Define local directory and SQLite database file\n",
        "local_dir = \"/content/drive/My Drive/nyc_taxi_data_2019\"\n",
        "db_file = \"nyc_taxi_data_2019.db\"\n",
        "\n",
        "# Function to process data\n",
        "def process_data(file_path, pickup_col, dropoff_col, distance_col, fare_col, passenger_col, taxi_type):\n",
        "    # Read parquet file into DataFrame\n",
        "    df = pd.read_parquet(file_path)\n",
        "\n",
        "    # Select necessary columns and drop missing values\n",
        "    df = df[[pickup_col, dropoff_col, distance_col, fare_col, passenger_col]].dropna()\n",
        "\n",
        "    # Rename columns\n",
        "    df.columns = ['pickup_datetime', 'dropoff_datetime', 'trip_distance', 'fare_amount', 'passenger_count']\n",
        "\n",
        "    # Convert datetime columns\n",
        "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
        "    df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])\n",
        "\n",
        "    # Derive new columns: trip duration (minutes) and average speed (mph)\n",
        "    df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
        "    df['average_speed'] = df['trip_distance'] / (df['trip_duration'] / 60)\n",
        "\n",
        "    # Remove invalid data\n",
        "    df = df[(df['trip_duration'] > 0) & (df['average_speed'].notnull())]\n",
        "\n",
        "    # Add taxi type column\n",
        "    df['taxi_type'] = taxi_type\n",
        "\n",
        "    # Aggregate data: total trips, average fare, and total passengers per day\n",
        "    df['pickup_date'] = df['pickup_datetime'].dt.date\n",
        "    agg_df = df.groupby('pickup_date').agg(\n",
        "        total_trips=('trip_distance', 'count'),\n",
        "        average_fare=('fare_amount', 'mean'),\n",
        "        total_passengers=('passenger_count', 'sum')\n",
        "    ).reset_index()\n",
        "\n",
        "    agg_df['taxi_type'] = taxi_type\n",
        "\n",
        "    # Drop the pickup_date column from the main dataframe to avoid insertion errors\n",
        "    df = df.drop(columns=['pickup_date'])\n",
        "\n",
        "    return df, agg_df\n",
        "\n",
        "# Function to load data into SQLite\n",
        "def load_to_sqlite(df, table_name, conn):\n",
        "    df.to_sql(table_name, conn, if_exists='append', index=False)\n",
        "\n",
        "# Create a connection to the SQLite database\n",
        "conn = sqlite3.connect(db_file)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Drop tables if they exist (optional)\n",
        "cursor.executescript('''\n",
        "DROP TABLE IF EXISTS Trips;\n",
        "DROP TABLE IF EXISTS DailyMetrics;\n",
        "''')\n",
        "\n",
        "# Create tables\n",
        "cursor.executescript('''\n",
        "-- Create Trips Table\n",
        "CREATE TABLE IF NOT EXISTS Trips (\n",
        "    trip_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    taxi_type TEXT,\n",
        "    pickup_datetime TIMESTAMP,\n",
        "    dropoff_datetime TIMESTAMP,\n",
        "    trip_distance REAL,\n",
        "    fare_amount REAL,\n",
        "    passenger_count INTEGER,\n",
        "    trip_duration REAL,\n",
        "    average_speed REAL\n",
        ");\n",
        "\n",
        "-- Create DailyMetrics Table\n",
        "CREATE TABLE IF NOT EXISTS DailyMetrics (\n",
        "    metric_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    taxi_type TEXT,\n",
        "    pickup_date DATE,\n",
        "    total_trips INTEGER,\n",
        "    average_fare REAL,\n",
        "    total_passengers INTEGER\n",
        ");\n",
        "''')\n",
        "\n",
        "# Process each file and load data into SQLite\n",
        "for month in ['01']:\n",
        "    for taxi_type, pickup_col, dropoff_col, distance_col, fare_col, passenger_col in [\n",
        "        ('yellow', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance', 'fare_amount', 'passenger_count'),\n",
        "        ('green', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'trip_distance', 'fare_amount', 'passenger_count'),\n",
        "        # ('fhvhv', 'pickup_datetime', 'dropoff_datetime', 'trip_miles', 'base_passenger_fare', 'passenger_count')\n",
        "    ]:\n",
        "        # Get file path\n",
        "        file_path = os.path.join(local_dir, f'{taxi_type}_tripdata_2019-{month}.parquet')\n",
        "\n",
        "        # Process data\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                df, agg_df = process_data(file_path, pickup_col, dropoff_col, distance_col, fare_col, passenger_col, taxi_type)\n",
        "\n",
        "                # Load processed data into SQLite\n",
        "                load_to_sqlite(df, 'Trips', conn)\n",
        "\n",
        "                # Load aggregated data into SQLite\n",
        "                load_to_sqlite(agg_df, 'DailyMetrics', conn)\n",
        "\n",
        "                print(f\"Data for {taxi_type} 2019-{month} loaded successfully.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_path}: {e}\")\n",
        "        else:\n",
        "            print(f\"File {file_path} does not exist.\")\n",
        "\n",
        "# Commit and close the SQLite connection\n",
        "conn.commit()\n",
        "conn.close()\n",
        "\n",
        "print(\"Data processing and loading to SQLite completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kivS-6IGbuui",
        "outputId": "c8364f40-7904-4086-cce5-a5c9319c0441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for yellow 2019-01 loaded successfully.\n",
            "Data for green 2019-01 loaded successfully.\n",
            "Data processing and loading to SQLite completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgLaSsasynIf"
      },
      "outputs": [],
      "source": []
    }
  ]
}