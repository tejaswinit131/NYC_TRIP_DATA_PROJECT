{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot5fbPkLKge7",
        "outputId": "b35dfd80-6b00-495f-d417-fce1f7501349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488490 sha256=47fd3ffd87cab55190f871cafa97f3e423ad6b488964b8ae29c6f4b8994d83dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RbiLwoLJzKXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TaxiDataProcessing\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "def clean_and_transform_spark(df, taxi_type):\n",
        "    if taxi_type == \"yellow\":\n",
        "        required_columns = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance', 'fare_amount']\n",
        "        df = df.dropna(subset=required_columns)\n",
        "        df = df.withColumn('pickup_datetime', F.to_timestamp('tpep_pickup_datetime')) \\\n",
        "               .withColumn('dropoff_datetime', F.to_timestamp('tpep_dropoff_datetime'))\n",
        "    elif taxi_type == \"green\":\n",
        "        required_columns = ['lpep_pickup_datetime', 'lpep_dropoff_datetime', 'trip_distance', 'fare_amount']\n",
        "        df = df.dropna(subset=required_columns)\n",
        "        df = df.withColumn('pickup_datetime', F.to_timestamp('lpep_pickup_datetime')) \\\n",
        "               .withColumn('dropoff_datetime', F.to_timestamp('lpep_dropoff_datetime'))\n",
        "    '''elif taxi_type == \"hvfht\":\n",
        "        required_columns = ['pickup_datetime', 'dropoff_datetime', 'trip_miles', 'base_passenger_fare']\n",
        "        df = df.dropna(subset=required_columns)\n",
        "        df = df.withColumn('pickup_datetime', F.to_timestamp('pickup_datetime')) \\\n",
        "               .withColumn('dropoff_datetime', F.to_timestamp('dropoff_datetime'))'''\n",
        "\n",
        "    df = df.withColumn('trip_duration',\n",
        "                       (F.col('dropoff_datetime').cast('long') - F.col('pickup_datetime').cast('long')) / 60) \\\n",
        "           .withColumn('average_speed', F.col('trip_distance') / (F.col('trip_duration') / 60))\n",
        "\n",
        "    return df\n",
        "\n",
        "# Define paths\n",
        "use_google_drive = True  # Set to False if using local upload\n",
        "if use_google_drive:\n",
        "    base_path = '/content/drive/My Drive/nyc_taxi_data_2019'# Update with your Google Drive path\n",
        "else:\n",
        "    base_path = '/content/'  # Default path for local uploads\n",
        "\n",
        "parquet_files = [\n",
        "    'file1.parquet',\n",
        "    'file2.parquet'\n",
        "]  # Update with your actual parquet file names\n",
        "\n",
        "# Read, clean, and display data from each parquet file\n",
        "for local_file in parquet_files:\n",
        "    file_path = os.path.join(base_path, local_file)\n",
        "    taxi_type = \"\"\n",
        "    if \"yellow\" in file_path:\n",
        "        taxi_type = \"yellow\"\n",
        "    elif \"green\" in file_path:\n",
        "        taxi_type = \"green\"\n",
        "    '''elif \"hvfht\" in file_path:\n",
        "        taxi_type = \"hvfht\"'''\n",
        "\n",
        "    if taxi_type:\n",
        "        try:\n",
        "            df = spark.read.parquet(file_path)\n",
        "            df = clean_and_transform_spark(df, taxi_type)\n",
        "            df.show(5)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {file_path}: {e}\")\n",
        "\n",
        "print(\"Data loaded, cleaned, and displayed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYi-KkdUkDPU",
        "outputId": "a4aab66f-578a-4699-87da-f889725fbcf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded, cleaned, and displayed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ANJgNqwczMBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Define local directory\n",
        "local_dir = \"/content/drive/My Drive/nyc_taxi_data_2019\"\n",
        "\n",
        "# Function to process data\n",
        "def process_data(file_path, pickup_col, dropoff_col, distance_col, fare_col):\n",
        "    # Read parquet file into DataFrame\n",
        "    df = pd.read_parquet(file_path)\n",
        "\n",
        "    # Select necessary columns and drop missing values\n",
        "    df = df[[pickup_col, dropoff_col, distance_col, fare_col]].dropna()\n",
        "\n",
        "    # Rename columns\n",
        "    df.columns = ['pickup_datetime', 'dropoff_datetime', 'trip_distance', 'fare_amount']\n",
        "\n",
        "    # Convert datetime columns\n",
        "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
        "    df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])\n",
        "\n",
        "    # Derive new columns: trip duration (minutes) and average speed (mph)\n",
        "    df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
        "    df['average_speed'] = df['trip_distance'] / (df['trip_duration'] / 60)\n",
        "\n",
        "    # Remove invalid data\n",
        "    df = df[(df['trip_duration'] > 0) & (df['average_speed'].notnull())]\n",
        "\n",
        "    # Aggregate data: total trips and average fare per day\n",
        "    df['pickup_date'] = df['pickup_datetime'].dt.date\n",
        "    agg_df = df.groupby('pickup_date').agg(\n",
        "        total_trips=('trip_distance', 'count'),\n",
        "        average_fare=('fare_amount', 'mean')\n",
        "    ).reset_index()\n",
        "\n",
        "    return df, agg_df\n",
        "\n",
        "# Process each file and display results\n",
        "for month in ['01']:\n",
        "    for taxi_type, pickup_col, dropoff_col, distance_col, fare_col in [\n",
        "        ('yellow', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance', 'fare_amount'),\n",
        "        ('green', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'trip_distance', 'fare_amount'),\n",
        "        # ('fhvhv', 'pickup_datetime', 'dropoff_datetime', 'trip_miles', 'base_passenger_fare')  # Removed extra values if any\n",
        "    ]:\n",
        "        # Get file path\n",
        "        file_path = os.path.join(local_dir, f'{taxi_type}_tripdata_2019-{month}.parquet')\n",
        "\n",
        "        # Process data\n",
        "        try:\n",
        "            df, agg_df = process_data(file_path, pickup_col, dropoff_col, distance_col, fare_col)\n",
        "\n",
        "            # Show DataFrame description\n",
        "            print(f\"Description of {taxi_type} data for 2019-{month}:\")\n",
        "            print(df.describe())\n",
        "\n",
        "            # Show aggregated results\n",
        "            print(agg_df.head())\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "print(\"Data processing completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSvzqMqceTPw",
        "outputId": "ad516299-106c-43b3-947b-c83e1a9b9ce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Description of yellow data for 2019-01:\n",
            "                  pickup_datetime            dropoff_datetime  trip_distance  \\\n",
            "count                     7690060                     7690060   7.690060e+06   \n",
            "mean   2019-01-17 00:59:01.944673  2019-01-17 01:15:36.753087   2.832390e+00   \n",
            "min           2001-02-02 14:55:07         2001-02-02 15:07:27   0.000000e+00   \n",
            "25%    2019-01-09 17:39:55.750000         2019-01-09 17:56:01   9.000000e-01   \n",
            "50%    2019-01-16 22:15:47.500000  2019-01-16 22:30:22.500000   1.540000e+00   \n",
            "75%           2019-01-24 19:12:24         2019-01-24 19:27:59   2.840000e+00   \n",
            "max           2088-01-24 00:25:39         2088-01-24 07:28:25   8.318000e+02   \n",
            "std                           NaN                         NaN   3.775024e+00   \n",
            "\n",
            "        fare_amount  trip_duration  average_speed  \n",
            "count  7.690060e+06   7.690060e+06   7.690060e+06  \n",
            "mean   1.243029e+01   1.658014e+01   1.318942e+01  \n",
            "min   -3.620000e+02   1.666667e-02   0.000000e+00  \n",
            "25%    6.000000e+00   6.116667e+00   7.594937e+00  \n",
            "50%    9.000000e+00   1.020000e+01   1.010204e+01  \n",
            "75%    1.350000e+01   1.666667e+01   1.377049e+01  \n",
            "max    6.232599e+05   4.364802e+04   7.344000e+04  \n",
            "std    2.250695e+02   7.516960e+01   1.374931e+02  \n",
            "  pickup_date  total_trips  average_fare\n",
            "0  2001-02-02            1      2.500000\n",
            "1  2003-01-01            1      0.000000\n",
            "2  2008-12-31           22     12.636364\n",
            "3  2009-01-01           48     13.510417\n",
            "4  2018-11-28           10     70.015000\n",
            "Description of green data for 2019-01:\n",
            "                  pickup_datetime            dropoff_datetime  trip_distance  \\\n",
            "count                      671386                      671386  671386.000000   \n",
            "mean   2019-01-16 17:03:34.481166  2019-01-16 17:26:09.894433       3.810468   \n",
            "min           2009-01-01 00:05:59         2009-01-01 00:17:01       0.000000   \n",
            "25%    2019-01-09 10:02:00.250000  2019-01-09 10:24:55.500000       1.100000   \n",
            "50%    2019-01-16 17:06:56.500000  2019-01-16 17:33:46.500000       2.130000   \n",
            "75%    2019-01-24 16:49:14.750000  2019-01-24 17:14:12.500000       4.730000   \n",
            "max           2019-02-01 21:51:23         2019-02-01 23:10:47     117.990000   \n",
            "std                           NaN                         NaN       4.372363   \n",
            "\n",
            "         fare_amount  trip_duration  average_speed  \n",
            "count  671386.000000  671386.000000  671386.000000  \n",
            "mean       15.643327      22.590221      14.350011  \n",
            "min      -120.000000       0.016667       0.000000  \n",
            "25%         6.500000       6.700000       8.850000  \n",
            "50%        10.500000      11.966667      11.300248  \n",
            "75%        19.290000      21.516667      15.165709  \n",
            "max       400.000000    1439.966667   54072.000000  \n",
            "std        13.935513      93.525019     131.185620  \n",
            "  pickup_date  total_trips  average_fare\n",
            "0  2009-01-01           12     16.083333\n",
            "1  2010-09-23           11      9.727273\n",
            "2  2018-12-04           10     15.745000\n",
            "3  2018-12-05           22     11.155455\n",
            "4  2018-12-07            3     43.000000\n",
            "Data processing completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vyB0X30mzL0f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}